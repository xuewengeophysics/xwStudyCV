# 深度学习可解释性笔记

[深度学习的可解释性研究可以分为两大类](https://www.zhihu.com/question/320688440)：

1. **深度学习模型理解**：在特定的任务中，用过可视化、参数分析对比实验去解释模型为什么work
   + CNN的feature map可视化
     + [utkuozbulak/pytorch-cnn-visualizations](utkuozbulak/pytorch-cnn-visualizations)
2. **深度学习理论**：利用计算学习理论，在不涉及任务背景、数据分布的情况下，提供深度学习模型的理论保证（模型的泛化能力、模型的capacity）



# [Exploring Explainability for Vision Transformers](https://jacobgil.github.io/deeplearning/vision-transformer-explainability)

## Background

### Explainability

+ *(useful for the developer)* **What’s going on inside when we run the Transformer on this image?**
+ *(useful for the developer*) **What did it learn?**
+ *(useful for both the developer and the user)* **What did it see in this image?**



Class Specific Explainability

some kind of a spatial continuity constraint between the patches here (and maybe also incorporate that into how the transformers process the images)





# [Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization](https://arxiv.org/abs/1610.02391)





[*Been Kim*: Introduction to Interpretable Machine Learning](http://deeplearning.csail.mit.edu/slide_cvpr2018/been_cvpr18tutorial.pdf)

Before building any model: Exploratory data analysis



